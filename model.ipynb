{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spam detection model using the vectorized data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To create a spam detection model using the vectorized data, we'll follow these steps:\n",
    "\n",
    "1. Label the Data: We need labels indicating whether each email is spam or not.\n",
    "2. Split the Data: Divide the data into training and testing sets to evaluate the model's performance accurately.\n",
    "3. Train a Model: Use a machine learning algorithm to train a model on the training data. Common choices for text classification include Logistic Regression, Random Forest, Naive Bayes, and Support Vector Machines (SVM).\n",
    "4. Evaluate the Model: Test the model on the testing set to assess its accuracy, recall, f1-score and confusion matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Loading the e-mail dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Subject</th>\n",
       "      <th>Content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>announcing sftp support for backup box</td>\n",
       "      <td>hi     thanks for using backup box  i wanted y...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>what s new  big android update  skitch sharing...</td>\n",
       "      <td>evernote newsletter  march 2012       in th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>boxbuzz  new onecloud unifies mobile apps with...</td>\n",
       "      <td>http   app en25 com e es aspx s 1464 e 28...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>kevin  see who you already know on linkedin</td>\n",
       "      <td>dear kevin  see who you alrea...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>3 99 album deals for nirvana  jay z  soundga...</td>\n",
       "      <td>google play    http   play google com       ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                                            Subject   \n",
       "0           0            announcing sftp support for backup box   \\\n",
       "1           1  what s new  big android update  skitch sharing...   \n",
       "2           2  boxbuzz  new onecloud unifies mobile apps with...   \n",
       "3           3        kevin  see who you already know on linkedin   \n",
       "4           4    3 99 album deals for nirvana  jay z  soundga...   \n",
       "\n",
       "                                             Content  \n",
       "0  hi     thanks for using backup box  i wanted y...  \n",
       "1     evernote newsletter  march 2012       in th...  \n",
       "2       http   app en25 com e es aspx s 1464 e 28...  \n",
       "3                   dear kevin  see who you alrea...  \n",
       "4    google play    http   play google com       ...  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the dataset\n",
    "file_path = r\"csv/mail.csv\"\n",
    "data = pd.read_csv(file_path)\n",
    "\n",
    "# Display the first few rows of the dataset to understand its structure\n",
    "data.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Vectorize the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To vectorize the dataset, we can use techniques like: <br>\n",
    "\n",
    " - Bag-of-Words (BoW): Represents text data by counting how many times each word appears.\n",
    " - TF-IDF (Term Frequency-Inverse Document Frequency): Weighs words according to their frequency across documents, giving higher importance to less common but potentially more relevant terms.\n",
    "\n",
    " We will use TF-IDF, which is commonly used for text processing as it helps in distinguishing more meaningful words from common ones. \n",
    "\n",
    "\n",
    "Focusing on the top 1000 terms to simplify the representation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1. Vectorize the dataset with TF-IDF (Term Frequency-Inverse Document Frequency)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>00</th>\n",
       "      <th>000</th>\n",
       "      <th>01</th>\n",
       "      <th>05</th>\n",
       "      <th>0px</th>\n",
       "      <th>10</th>\n",
       "      <th>100</th>\n",
       "      <th>10012</th>\n",
       "      <th>102</th>\n",
       "      <th>11</th>\n",
       "      <th>...</th>\n",
       "      <th>writing</th>\n",
       "      <th>www</th>\n",
       "      <th>year</th>\n",
       "      <th>years</th>\n",
       "      <th>yet</th>\n",
       "      <th>york</th>\n",
       "      <th>you</th>\n",
       "      <th>your</th>\n",
       "      <th>yourself</th>\n",
       "      <th>youtube</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.121157</td>\n",
       "      <td>0.015350</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.051946</td>\n",
       "      <td>0.122855</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.02228</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.024515</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.051266</td>\n",
       "      <td>0.040902</td>\n",
       "      <td>0.088839</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.029388</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.154238</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.244195</td>\n",
       "      <td>0.025114</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.035385</td>\n",
       "      <td>0.035866</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1627</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.054336</td>\n",
       "      <td>0.220298</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1628</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.109635</td>\n",
       "      <td>0.022225</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1629</th>\n",
       "      <td>0.108243</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.066234</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.538199</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.025862</td>\n",
       "      <td>0.078639</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1630</th>\n",
       "      <td>0.108175</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.066193</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.537864</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.025845</td>\n",
       "      <td>0.078590</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1631</th>\n",
       "      <td>0.105673</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.064662</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.525422</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.037871</td>\n",
       "      <td>0.063977</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1632 rows × 1000 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            00  000   01   05  0px        10      100  10012  102        11   \n",
       "0     0.000000  0.0  0.0  0.0  0.0  0.000000  0.00000    0.0  0.0  0.000000  \\\n",
       "1     0.000000  0.0  0.0  0.0  0.0  0.000000  0.00000    0.0  0.0  0.000000   \n",
       "2     0.000000  0.0  0.0  0.0  0.0  0.000000  0.02228    0.0  0.0  0.000000   \n",
       "3     0.000000  0.0  0.0  0.0  0.0  0.000000  0.00000    0.0  0.0  0.000000   \n",
       "4     0.000000  0.0  0.0  0.0  0.0  0.000000  0.00000    0.0  0.0  0.000000   \n",
       "...        ...  ...  ...  ...  ...       ...      ...    ...  ...       ...   \n",
       "1627  0.000000  0.0  0.0  0.0  0.0  0.000000  0.00000    0.0  0.0  0.000000   \n",
       "1628  0.000000  0.0  0.0  0.0  0.0  0.000000  0.00000    0.0  0.0  0.000000   \n",
       "1629  0.108243  0.0  0.0  0.0  0.0  0.066234  0.00000    0.0  0.0  0.538199   \n",
       "1630  0.108175  0.0  0.0  0.0  0.0  0.066193  0.00000    0.0  0.0  0.537864   \n",
       "1631  0.105673  0.0  0.0  0.0  0.0  0.064662  0.00000    0.0  0.0  0.525422   \n",
       "\n",
       "      ...   writing       www      year  years  yet      york       you   \n",
       "0     ...  0.000000  0.000000  0.000000    0.0  0.0  0.000000  0.121157  \\\n",
       "1     ...  0.000000  0.000000  0.000000    0.0  0.0  0.000000  0.051946   \n",
       "2     ...  0.024515  0.000000  0.000000    0.0  0.0  0.051266  0.040902   \n",
       "3     ...  0.000000  0.000000  0.000000    0.0  0.0  0.000000  0.154238   \n",
       "4     ...  0.000000  0.244195  0.025114    0.0  0.0  0.000000  0.035385   \n",
       "...   ...       ...       ...       ...    ...  ...       ...       ...   \n",
       "1627  ...  0.000000  0.000000  0.000000    0.0  0.0  0.000000  0.054336   \n",
       "1628  ...  0.000000  0.000000  0.000000    0.0  0.0  0.000000  0.109635   \n",
       "1629  ...  0.000000  0.000000  0.000000    0.0  0.0  0.000000  0.025862   \n",
       "1630  ...  0.000000  0.000000  0.000000    0.0  0.0  0.000000  0.025845   \n",
       "1631  ...  0.000000  0.000000  0.000000    0.0  0.0  0.000000  0.037871   \n",
       "\n",
       "          your  yourself   youtube  \n",
       "0     0.015350       0.0  0.000000  \n",
       "1     0.122855       0.0  0.000000  \n",
       "2     0.088839       0.0  0.029388  \n",
       "3     0.000000       0.0  0.000000  \n",
       "4     0.035866       0.0  0.000000  \n",
       "...        ...       ...       ...  \n",
       "1627  0.220298       0.0  0.000000  \n",
       "1628  0.022225       0.0  0.000000  \n",
       "1629  0.078639       0.0  0.000000  \n",
       "1630  0.078590       0.0  0.000000  \n",
       "1631  0.063977       0.0  0.000000  \n",
       "\n",
       "[1632 rows x 1000 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Initialize the TF-IDF Vectorizer\n",
    "tfidf_vectorizer = TfidfVectorizer(max_features=1000)  # Limit to top 1000 features for simplicity\n",
    "\n",
    "# Fit and transform the 'Content' column\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform(data['Content'])\n",
    "\n",
    "# Create a DataFrame to show the TF-IDF features for the first few rows\n",
    "tfidf_df = pd.DataFrame(tfidf_matrix.toarray(), columns=tfidf_vectorizer.get_feature_names_out())\n",
    "tfidf_df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "pkl_filename = 'model/prog8245nlp-tfidf.pkl'\n",
    "with open(pkl_filename, 'wb') as file:\n",
    "    pickle.dump(tfidf_vectorizer, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2. Saving the Vectorize the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "tfidf_df.to_csv(r'csv/tfidf_vectorized_data.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(    00  000   01   05  0px   10      100  10012  102   11  ...   writing   \n",
       " 0  0.0  0.0  0.0  0.0  0.0  0.0  0.00000    0.0  0.0  0.0  ...  0.000000  \\\n",
       " 1  0.0  0.0  0.0  0.0  0.0  0.0  0.00000    0.0  0.0  0.0  ...  0.000000   \n",
       " 2  0.0  0.0  0.0  0.0  0.0  0.0  0.02228    0.0  0.0  0.0  ...  0.024515   \n",
       " 3  0.0  0.0  0.0  0.0  0.0  0.0  0.00000    0.0  0.0  0.0  ...  0.000000   \n",
       " 4  0.0  0.0  0.0  0.0  0.0  0.0  0.00000    0.0  0.0  0.0  ...  0.000000   \n",
       " \n",
       "         www      year  years  yet      york       you      your  yourself   \n",
       " 0  0.000000  0.000000    0.0  0.0  0.000000  0.121157  0.015350       0.0  \\\n",
       " 1  0.000000  0.000000    0.0  0.0  0.000000  0.051946  0.122855       0.0   \n",
       " 2  0.000000  0.000000    0.0  0.0  0.051266  0.040902  0.088839       0.0   \n",
       " 3  0.000000  0.000000    0.0  0.0  0.000000  0.154238  0.000000       0.0   \n",
       " 4  0.244195  0.025114    0.0  0.0  0.000000  0.035385  0.035866       0.0   \n",
       " \n",
       "     youtube  \n",
       " 0  0.000000  \n",
       " 1  0.000000  \n",
       " 2  0.029388  \n",
       " 3  0.000000  \n",
       " 4  0.000000  \n",
       " \n",
       " [5 rows x 1000 columns],\n",
       " Index(['00', '000', '01', '05', '0px', '10', '100', '10012', '102', '11',\n",
       "        ...\n",
       "        'writing', 'www', 'year', 'years', 'yet', 'york', 'you', 'your',\n",
       "        'yourself', 'youtube'],\n",
       "       dtype='object', length=1000))"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the vectorized data\n",
    "file_path = r'csv/tfidf_vectorized_data.csv'\n",
    "vectorized_data = pd.read_csv(file_path)\n",
    "\n",
    "# Display the first few rows of the dataset and its structure\n",
    "vectorized_data.head(), vectorized_data.columns\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3.Experiment with 3 different feature extraction techniques to capture meaningful representations of social media text where the 3 techniques should be of different word embedding categories."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To explore different techniques for capturing meaningful representations of social media text, we can experiment with three distinct types of word embedding categories: \n",
    "\n",
    "- count-based: TF-IDF Vectorization (already implemented item 2.1.)<br>\n",
    "TF-IDF is effective for highlighting important words that are frequent in a few documents\n",
    "\n",
    "- predictive: Word2Vec\n",
    "\n",
    "\n",
    "-  sparse Matrix Factorization: Non-negative Matrix Factorization (NMF)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3.1. Predictive Model: Word2Vec\n",
    "Word2Vec is a predictive model that uses neural networks to learn word associations from a large corpus of text. After training, the model can detect synonymous words or suggest additional words for a partial sentence. To implement this, we can use the gensim library:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gensim in c:\\users\\arcad\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (4.3.2)\n",
      "Requirement already satisfied: numpy>=1.18.5 in c:\\users\\arcad\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from gensim) (1.26.3)\n",
      "Requirement already satisfied: scipy>=1.7.0 in c:\\users\\arcad\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from gensim) (1.11.4)\n",
      "Requirement already satisfied: smart-open>=1.8.1 in c:\\users\\arcad\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from gensim) (7.0.4)\n",
      "Requirement already satisfied: wrapt in c:\\users\\arcad\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from smart-open>=1.8.1->gensim) (1.16.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.3.2 -> 24.0\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         0         1         2         3         4         5         6    \n",
      "0 -0.132779 -0.306009  0.218971  0.886176 -0.039734 -0.740119  0.057367  \\\n",
      "1 -0.076492 -0.047740  0.336058  0.791808 -0.142548 -0.761013  0.090558   \n",
      "2 -0.036042 -0.010476  0.353221  0.893480 -0.113499 -0.728073  0.105870   \n",
      "3  0.046882 -0.245239  0.039179  1.019770 -0.045665 -0.846046  0.473050   \n",
      "4 -0.314919 -0.301436  0.112026  0.520614 -0.506260 -1.147483 -0.366100   \n",
      "\n",
      "         7         8         9   ...        90        91        92        93   \n",
      "0  1.001050  0.035703  0.231131  ...  0.276000  0.299138  0.052639  0.344628  \\\n",
      "1  1.025684 -0.084030  0.079842  ...  0.236329  0.227532 -0.049899  0.182343   \n",
      "2  1.228045 -0.190517  0.022867  ...  0.100347  0.200697  0.003069  0.212455   \n",
      "3  1.562357 -0.100602  0.465762  ...  0.506108  0.517001  0.210131 -0.034123   \n",
      "4  0.700114  0.094000  0.401149  ...  0.529058  0.460223 -0.045456 -0.408327   \n",
      "\n",
      "         94        95        96        97        98        99  \n",
      "0  0.662494  0.073448 -0.567600 -0.206286 -0.096539 -0.391846  \n",
      "1  0.658638  0.017336 -0.585024 -0.225927 -0.132008 -0.313596  \n",
      "2  0.757585  0.038762 -0.603414 -0.256706 -0.187982 -0.194178  \n",
      "3  0.998241  0.154781 -0.544236 -0.380229 -0.074763 -0.357657  \n",
      "4  0.624215  0.004278 -0.646681 -0.134240 -0.137005 -0.732526  \n",
      "\n",
      "[5 rows x 100 columns]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np  # Make sure to import numpy\n",
    "import gensim\n",
    "from gensim.models import Word2Vec\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Tokenize the text\n",
    "data['tokenized'] = data['Content'].apply(word_tokenize)\n",
    "\n",
    "# Train a Word2Vec model\n",
    "model_w2v = Word2Vec(sentences=data['tokenized'], vector_size=100, window=5, min_count=1, workers=4)\n",
    "\n",
    "# Function to average all word vectors in a paragraph\n",
    "def document_vector(doc):\n",
    "    # Remove out-of-vocabulary words\n",
    "    doc = [word for word in doc if word in model_w2v.wv.key_to_index]\n",
    "    return np.mean(model_w2v.wv[doc], axis=0) if doc else np.zeros((100,))\n",
    "\n",
    "# Apply the function to each document\n",
    "data['doc_vector'] = data['tokenized'].apply(document_vector)\n",
    "w2v_df = pd.DataFrame(data['doc_vector'].tolist())\n",
    "\n",
    "print(w2v_df.head())  # Displays the first five rows of the Word2Vec DataFrame2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3.2 Sparse Matrix Factorization: Non-negative Matrix Factorization (NMF)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Non-negative Matrix Factorization (NMF) is indeed used in combination with a TF-IDF matrix. In text mining, NMF is commonly applied to matrices derived from count-based techniques.\n",
    "\n",
    "NMF belongs to a group of algorithms that perform matrix factorization under non-negativity constraints, commonly used for dimensionality reduction and feature extraction, particularly in the context of discovering latent topics or structures within data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         0         1         2         3         4    5         6         7    \n",
      "0  0.000000  0.005815  0.000000  0.002579  0.000000  0.0  0.000606  0.000000  \\\n",
      "1  0.000000  0.000000  0.013166  0.000000  0.000000  0.0  0.000000  0.000000   \n",
      "2  0.000000  0.000000  0.003473  0.002369  0.000000  0.0  0.000000  0.000000   \n",
      "3  0.000765  0.000000  0.000000  0.000000  0.000000  0.0  0.000000  0.000000   \n",
      "4  0.002852  0.000000  0.000000  0.000000  0.186151  0.0  0.000000  0.000026   \n",
      "\n",
      "    8         9   ...       40        41        42        43        44   \n",
      "0  0.0  0.000000  ...  0.00016  0.000000  0.001891  0.048537  0.132014  \\\n",
      "1  0.0  0.000000  ...  0.00000  0.005964  0.012670  0.056631  0.118135   \n",
      "2  0.0  0.000000  ...  0.00000  0.000000  0.000000  0.078932  0.072407   \n",
      "3  0.0  0.000386  ...  0.00000  0.000000  0.000000  0.027284  0.097642   \n",
      "4  0.0  0.000000  ...  0.00000  0.000000  0.000000  0.042866  0.000000   \n",
      "\n",
      "         45        46        47        48        49  \n",
      "0  0.010202  0.001818  0.000000  0.000885  0.000000  \n",
      "1  0.000000  0.000000  0.000000  0.000000  0.000526  \n",
      "2  0.000000  0.000000  0.000000  0.000121  0.010781  \n",
      "3  0.000000  0.000000  0.000000  0.043778  0.000000  \n",
      "4  0.000000  0.000000  0.049686  0.000000  0.000000  \n",
      "\n",
      "[5 rows x 50 columns]\n",
      "            0         1    2         3    4    5         6    7         8    \n",
      "1627  0.000626  0.013396  0.0  0.002764  0.0  0.0  0.001125  0.0  0.000000  \\\n",
      "1628  0.000000  0.003131  0.0  0.000000  0.0  0.0  0.000526  0.0  0.002161   \n",
      "1629  0.000000  0.004209  0.0  0.000000  0.0  0.0  0.000000  0.0  0.028598   \n",
      "1630  0.000000  0.004349  0.0  0.000000  0.0  0.0  0.000000  0.0  0.028515   \n",
      "1631  0.000000  0.008746  0.0  0.000000  0.0  0.0  0.000000  0.0  0.028852   \n",
      "\n",
      "       9   ...        40   41   42        43        44        45        46   \n",
      "1627  0.0  ...  0.000000  0.0  0.0  0.004978  0.127718  0.004058  0.025500  \\\n",
      "1628  0.0  ...  0.000046  0.0  0.0  0.118619  0.126089  0.002527  0.001047   \n",
      "1629  0.0  ...  0.000361  0.0  0.0  0.000000  0.000000  0.000000  0.000000   \n",
      "1630  0.0  ...  0.000345  0.0  0.0  0.000000  0.000000  0.000000  0.000000   \n",
      "1631  0.0  ...  0.001637  0.0  0.0  0.000000  0.000000  0.000000  0.002275   \n",
      "\n",
      "           47        48        49  \n",
      "1627  0.00000  0.000000  0.035520  \n",
      "1628  0.04174  0.007281  0.008871  \n",
      "1629  0.00000  0.000000  0.008061  \n",
      "1630  0.00000  0.000000  0.009160  \n",
      "1631  0.00000  0.000000  0.004070  \n",
      "\n",
      "[5 rows x 50 columns]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import NMF\n",
    "\n",
    "# Initialize the TF-IDF Vectorizer\n",
    "tfidf_vectorizer = TfidfVectorizer(max_features=1000)\n",
    "\n",
    "# Fit and transform the 'Content' column\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform(data['Content'])\n",
    "\n",
    "# Apply NMF to reduce dimensionality and capture context\n",
    "nmf = NMF(n_components=50, random_state=42)  # You can adjust the number of components\n",
    "nmf_features = nmf.fit_transform(tfidf_matrix)\n",
    "\n",
    "# Optionally convert to DataFrame\n",
    "nmf_df = pd.DataFrame(nmf_features)\n",
    "\n",
    "\n",
    "# Display the first 5 rows of the DataFrame\n",
    "print(nmf_df.head())\n",
    "\n",
    "# Display the last 5 rows of the DataFrame\n",
    "print(nmf_df.tail())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Label the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Labeling data for spam detection. Here are some general guidelines and approaches we might consider:\n",
    "\n",
    "1. Manual Labeling\n",
    "If the dataset is not too large, we could manually review each email (or document) and assign a label based on its content:<br>\n",
    "Spam (1): Emails that are unsolicited and typically trying to sell something, phishing attempts, or contain malicious links.<br>\n",
    "Not Spam (0): Legitimate emails that are typically personal, business-related, or relevant communications you expect to receive.\n",
    "\n",
    "2. Using Pre-labeled Data\n",
    "If manually labeling the dataset is impractical due to its size or complexity, consider using a pre-labeled dataset:<br>\n",
    "Many public datasets are available for spam detection, such as the Enron Corpus, SpamAssassin Public Corpus, etc.<br>\n",
    "You can train your model on a pre-labeled dataset and use your unlabeled data as additional testing or real-world application data.\n",
    "\n",
    "3. Automated Labeling with Rules\n",
    "For larger datasets, you might develop rules to automatically assign labels based on:<br>\n",
    "Keywords: Spam often contains specific keywords (e.g., \"free\", \"winner\", \"urgent\", \"risk-free\").<br>\n",
    "Sender's Email Address: Emails from certain domains or with suspicious patterns might be more likely to be spam.<br>\n",
    "Formatting and Presentation: Use of excessive capitalization, multiple font colors, or poor formatting can be indicators of spam."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to use our own manual labeling data "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1.1. Download from our github\n",
    "\n",
    "https://github.com/onlyxool/PROG8245NLP\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the dataset\n",
    "file_path = r\"csv/mail_labeling.csv\"\n",
    "data_label = pd.read_csv(file_path)\n",
    "\n",
    "# Display the first few rows of the dataset to understand its structure\n",
    "# data_label.head()\n",
    "data_label\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1.2. Vectorize labeling data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How the `tfidf_vectorized_data` already encompasses a comprehensive vectorization of both the Subject and Content, then additional vectorization would not be necessary and could be redundant. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Split the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "\n",
    "# Convert labels to numeric format (1 for Spam, 0 for Not Spam)\n",
    "data_label['label_numeric'] = (data_label['label'] == 'Spam').astype(int)\n",
    "\n",
    "# Split data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(vectorized_data, data_label['label_numeric'], test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Train a Model - Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Split data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(vectorized_data, data_label['label_numeric'], test_size=0.2, random_state=42)\n",
    "\n",
    "# Train a Logistic Regression model\n",
    "model = LogisticRegression(max_iter=1000)  # Increased max_iter for convergence\n",
    "\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Predict on the testing set\n",
    "y_pred = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pkl_filename = 'model/prog8245nlp.pkl'\n",
    "with open(pkl_filename, 'wb') as file:\n",
    "    pickle.dump(model, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Evaluate the model: Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.1. Accuracy, recall, f1-score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "import pandas as pd\n",
    "\n",
    "# Calculate and display accuracy and other metrics\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "report = classification_report(y_test, y_pred)\n",
    "\n",
    "print(f\"Accuracy: {accuracy}\\n\")\n",
    "print(report)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Precision: Precision measures the proportion of correctly identified positive cases (spam emails) out of all cases classified as positive. For spam class (Spam), it's 0.76, meaning 76% of the emails classified as spam are actually spam.\n",
    "\n",
    "Recall: Recall measures the proportion of correctly identified positive cases (spam emails) out of all actual positive cases. For spam class (Spam), it's 0.99, indicating that the model captures 99% of the actual spam emails.\n",
    "\n",
    "F1-score: The F1-score is the harmonic mean of precision and recall, providing a balance between the two metrics. For spam class (Spam), it's 0.86, reflecting a good balance between precision and recall.\n",
    "\n",
    "Support: Support refers to the number of actual occurrences of each class in the test dataset. For spam class (Spam), there are 246 instances in the test dataset.\n",
    "\n",
    "The support value for \"0\" (non-spam emails) in the classification report indicates that out of the 327 emails in the test set, 81 were classified as \"0\" (non-spam emails). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.2. Confusion matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Matrix confision](https://miro.medium.com/v2/resize:fit:640/format:webp/1*Z54JgbS4DUwWSknhDCvNTQ.png)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# Assuming y_test and y_pred are defined and contain the true and predicted labels respectively\n",
    "conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "# Create a pandas DataFrame from the confusion matrix for easier plotting\n",
    "conf_matrix_df_lr = pd.DataFrame(conf_matrix,columns=['Positive', 'Negative'],\n",
    "                              index=['Positive', 'Negative'])\n",
    "\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "\n",
    "# Plot the confusion matrix as a heatmap\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(conf_matrix_df_lr, annot=True, fmt='d', cmap='Blues')\n",
    "plt.title('Confusion Matrix - Logistic Regression')\n",
    "plt.ylabel('Predicted Values')\n",
    "plt.xlabel('Actual Values')\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Top Left (Purple): True Positive (TP) - The number of positive instances correctly predicted by the model as positive. In this case, there are 4.\n",
    "\n",
    "Top Right (Blue): False Negative (FN) - The number of positive instances incorrectly predicted by the model as negative. There are 77, indicating these were spam emails that the model failed to identify correctly.\n",
    "\n",
    "Bottom Left (Purple): False Positive (FP) - The number of negative instances incorrectly predicted by the model as positive. The model has identified 2 such cases.\n",
    "\n",
    "Bottom Right (Yellow): True Negative (TN) - The number of negative instances correctly predicted by the model as negative. There are 244, indicating these were non-spam emails that the model correctly identified."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. Train a Model - RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Split data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(vectorized_data, data_label['label_numeric'], test_size=0.2, random_state=42)\n",
    "\n",
    "# Train a Random Forest model\n",
    "model = RandomForestClassifier(n_estimators=100, random_state=42)  # You can adjust the number of trees (n_estimators)\n",
    "\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Predict on the testing set\n",
    "y_pred= model.predict(X_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8. Evaluate the model: RandomForestClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8.1. Accuracy, recall, f1-score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "import pandas as pd\n",
    "\n",
    "# Calculate and display accuracy and other metrics\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "report_rf = classification_report(y_test, y_pred)\n",
    "\n",
    "print(f\"Accuracy: {accuracy}\\n\")\n",
    "print(report_rf)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8.2. Confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# Assuming y_test and y_pred are defined and contain the true and predicted labels respectively\n",
    "conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "# Create a pandas DataFrame from the confusion matrix for easier plotting\n",
    "conf_matrix_df_rf = pd.DataFrame(conf_matrix,columns=['Positive', 'Negative'],\n",
    "                              index=['Positive', 'Negative'])\n",
    "\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "\n",
    "# Plot the confusion matrix as a heatmap\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(conf_matrix_df_rf, annot=True, fmt='d', cmap='Blues')\n",
    "plt.title('Confusion Matrix - Random Forest')\n",
    "plt.ylabel('Predicted Values')\n",
    "plt.xlabel('Actual Values')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Top-left square (True Positives, TP): The number of positive instances correctly predicted by the model (23).\n",
    "- Top-right square (False Negatives, FN): The number of positive instances incorrectly predicted as negative by the model (58).\n",
    "- Bottom-left square (False Positives, FP): The number of negative instances incorrectly predicted as positive by the model (19).\n",
    "- Bottom-right square (True Negatives, TN): The number of negative instances correctly predicted by the model (227)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 10.Which confusion matrix is better?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Assuming you already have the confusion matrices conf_matrix_lr and conf_matrix_rf\n",
    "\n",
    "# Convert confusion matrices to pandas DataFrames\n",
    "conf_matrix_df_lr = pd.DataFrame(conf_matrix_df_lr, columns=['Positive', 'Negative'], index=['Positive', 'Negative'])\n",
    "conf_matrix_df_rf = pd.DataFrame(conf_matrix_df_rf, columns=['Positive', 'Negative'], index=['Positive', 'Negative'])\n",
    "\n",
    "# Create a figure with two subplots\n",
    "fig, axs = plt.subplots(1, 2, figsize=(12, 6))\n",
    "\n",
    "# Plot confusion matrix for logistic regression\n",
    "sns.heatmap(conf_matrix_df_lr, annot=True, fmt='d', cmap='Blues', cbar=False, ax=axs[0])\n",
    "axs[0].set_title('Logistic Regression')\n",
    "\n",
    "# Plot confusion matrix for random forest\n",
    "sns.heatmap(conf_matrix_df_rf, annot=True, fmt='d', cmap='Blues', cbar=False, ax=axs[1])\n",
    "axs[1].set_title('Random Forest')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the Logistic Regression model:\n",
    "\n",
    "- True Positives (TP): 4\n",
    "- False Negatives (FN): 77\n",
    "- False Positives (FP): 2\n",
    "- True Negatives (TN): 244\n",
    "\n",
    "For the Random Forest model:\n",
    "\n",
    "- True Positives (TP): 23\n",
    "- False Negatives (FN): 58\n",
    "- False Positives (FP): 19\n",
    "- True Negatives (TN): 227"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Logistic Regression:\\n {report}\")\n",
    "print(f\"Randon Forest:\\n {report_rf}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Precision: Logistic Regression is more precise than Random Forest, meaning it has a lower rate of false positives.\n",
    "- Recall: Random Forest has a higher recall than Logistic Regression, meaning it is better at identifying positive instances.\n",
    "- Accuracy: Both models have similar accuracy, with Random Forest being slightly higher.\n",
    "- F1 Score: Random Forest has a much higher F1 Score than Logistic Regression, indicating a better balance between precision and recall.\n",
    "\n",
    "Considering these factors, the Random Forest model seems to perform better overall.<br>\n",
    "It has significantly better recall and F1 score, indicating that it is more effective at classifying positive instances without compromising too much on precision.<br> \n",
    "The slightly higher accuracy of the Random Forest model also supports this conclusion."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "CSCN8010_classic_ml",
   "language": "python",
   "name": "cscn8010_classic_ml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
